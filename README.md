# Sign Language Detection Using Deep Learning and Computer Vision

## ğŸ“Œ Project Overview
This project aims to **detect and classify sign language gestures** using **Computer Vision** and a **Convolutional Neural Network (CNN)**.  
The model processes images or real-time webcam input to identify hand gestures, making it useful for accessibility, communication, and educational applications.

---

## âœ¨ Features
- **Real-time Detection:** Live hand gesture recognition using a webcam.  
- **CNN-Based Model:** Deep learning model for accurate classification.  
- **Multiple Classes:** Supports customizable sign language gestures.  
- **Preprocessing Pipeline:** Includes hand tracking and image preprocessing.  
- **Performance Metrics:** Evaluated using accuracy, confusion matrix, and balanced accuracy.  

---

## ğŸ›  Technologies Used
- **Python 3.x**
- **OpenCV** â€“ Image and video processing  
- **Mediapipe / cvzone** â€“ Hand tracking & keypoint extraction  
- **TensorFlow / Keras** â€“ CNN model training & inference  
- **NumPy & Pandas** â€“ Data handling  
- **Matplotlib / Seaborn** â€“ Performance visualization  

---

## âš™ï¸ Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/hgusweldeyowhanes/Sign-Language-Detection.git
   cd Sign-Language-Detection
   ```
2. Create and activate a virtual environment:
 ```bash
 python -m venv venv
# Linux/Mac
source venv/bin/activate
# Windows
venv\Scripts\activate
```
3. Install dependencies:
```bash
pip install -r requirements.txt
```
## Dataset
The dataset consists of images of hand gestures representing different classes of sign language.
```bash
Data/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ CLASS_1/
â”‚   â”œâ”€â”€ CLASS_2/
â”‚   â””â”€â”€ CLASS_3/
â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ CLASS_1/
â”‚   â”œâ”€â”€ CLASS_2/
â”‚   â””â”€â”€ CLASS_3/
â””â”€â”€ validation/
    â”œâ”€â”€ CLASS_1/
    â”œâ”€â”€ CLASS_2/
    â””â”€â”€ CLASS_3/

```

##ğŸ§ Model Architecture
Input Layer: 224Ã—224 RGB images

Convolutional Layers: Multiple Conv2D layers with ReLU activation

Pooling Layers: MaxPooling2D for dimensionality reduction

Dropout Layers: Prevents overfitting

Output Layer: Softmax classifier (number of gesture classes)

ğŸ“Š Performance
Accuracy: ~95â€“100% (depending on dataset & training)

Confusion Matrix: Visualizes misclassifications

Balanced Accuracy: Effective with class imbalance

ğŸš€ Usage

Run real-time detection with webcam:
```bash
jupyter notebook Prediction.ipynb
```
ğŸ“Œ Future Improvements

Expand dataset with more sign language gestures

Add multilingual sign language support

Deploy as a web or mobile app